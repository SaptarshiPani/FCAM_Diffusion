{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9949741,"sourceType":"datasetVersion","datasetId":6118595}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.distributions as dist\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\nimport torchvision\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nfrom PIL import Image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(self, input_channels=4, output_channels=1):\n        super(UNet, self).__init__()\n        \n        # Encoder\n        self.enc1 = self.conv_block(input_channels, 64)\n        self.enc2 = self.conv_block(64, 128)\n        self.enc3 = self.conv_block(128, 256)\n        self.enc4 = self.conv_block(256, 512)\n        \n        # Bottleneck\n        self.bottleneck = self.conv_block(512, 1024)\n        \n        # Decoder\n        self.up4 = self.upconv(1024, 512)\n        self.dec4 = self.conv_block(1024, 512)\n        \n        self.up3 = self.upconv(512, 256)\n        self.dec3 = self.conv_block(512, 256)\n        \n        self.up2 = self.upconv(256, 128)\n        self.dec2 = self.conv_block(256, 128)\n        \n        self.up1 = self.upconv(128, 64)\n        self.dec1 = self.conv_block(128, 64)\n        \n        self.final = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n        self.final2 = nn.Conv2d(3, output_channels, kernel_size=1)\n        self.out_act = nn.Sigmoid()\n        \n    def conv_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n    \n    def upconv(self, in_channels, out_channels):\n        return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n    \n    def forward(self, x, condition):\n        # Concatenate condition with input (conditioning)\n        \n        y = torch.cat((x, condition), dim=1)\n        #print(x.shape)\n        \n        # Encoder\n        enc1 = self.enc1(y)\n        enc2 = self.enc2(F.max_pool2d(enc1, 2))\n        enc3 = self.enc3(F.max_pool2d(enc2, 2))\n        enc4 = self.enc4(F.max_pool2d(enc3, 2))\n        \n        # Bottleneck\n        bottleneck = self.bottleneck(F.max_pool2d(enc4, 2))\n        \n        # Decoder\n        up4 = self.up4(bottleneck)\n        dec4 = self.dec4(torch.cat((up4, enc4), dim=1))\n        \n        up3 = self.up3(dec4)\n        dec3 = self.dec3(torch.cat((up3, enc3), dim=1))\n        \n        up2 = self.up2(dec3)\n        dec2 = self.dec2(torch.cat((up2, enc2), dim=1))\n        \n        up1 = self.up1(dec2)\n        dec1 = self.dec1(torch.cat((up1, enc1), dim=1))\n        \n        return self.out_act(self.final2(x-self.final(dec1)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Diffusion:\n    def __init__(self, T=1000, beta_start=1e-4, beta_end=0.02, device=False):\n        self.device = device  # Store device information (cpu or cuda)\n        self.T = T  # Total timesteps\n        self.betas = torch.linspace(beta_start, beta_end, T)  # Noise schedule\n        self.alphas = 1.0 - self.betas\n        self.alpha_hat = torch.cumprod(self.alphas, dim=0).to(device)\n    \n    def q_sample(self, x_start, t, noise=None):\n        \"\"\"Add noise to input at timestep t.\"\"\"\n        if noise is None:\n            noise = torch.randn_like(x_start)\n        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n        return sqrt_alpha_hat * x_start + sqrt_one_minus_alpha_hat * noise, noise","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SegmentationDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, transform=None, target_transform=None):\n        \"\"\"\n        Args:\n            image_dir (str): Path to the folder containing input images.\n            mask_dir (str): Path to the folder containing segmentation masks.\n            transform: Transformations for the input images.\n            target_transform: Transformations for the masks.\n        \"\"\"\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.image_filenames = sorted(os.listdir(image_dir))  # Sort to match images and masks\n        self.mask_filenames = sorted(os.listdir(mask_dir))\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __len__(self):\n        return len(self.image_filenames)\n\n    def __getitem__(self, idx):\n        # Load image and mask\n        image_path = os.path.join(self.image_dir, self.image_filenames[idx])\n        mask_path = os.path.join(self.mask_dir, self.mask_filenames[idx])\n        \n        image = Image.open(image_path).convert(\"RGB\")  # Convert to RGB\n        mask = Image.open(mask_path).convert(\"L\")     # Convert mask to grayscale\n        \n        # Apply transformations\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            mask = self.target_transform(mask)\n        \n        return image, mask\n\n# Define transformations for images and masks\nimage_transform = transforms.Compose([\n    transforms.Resize((128, 128)),  # Resize images to a fixed size\n    transforms.ToTensor(),          # Convert images to PyTorch tensors\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize images\n])\n\nmask_transform = transforms.Compose([\n    transforms.Resize((128, 128)),  # Resize masks to match input size\n    transforms.ToTensor(),          # Convert masks to PyTorch tensors\n])\n\n# Paths to dataset folders\ntrain_image_dir = \"/kaggle/input/brest-cancer-datsets/MonuSeg/MonuSeg/Training/Images\"\ntrain_mask_dir = \"/kaggle/input/brest-cancer-datsets/MonuSeg/MonuSeg/Training/Masks\"\ntest_image_dir = \"/kaggle/input/brest-cancer-datsets/MonuSeg/MonuSeg/Test/Images\"\ntest_mask_dir = \"/kaggle/input/brest-cancer-datsets/MonuSeg/MonuSeg/Test/Masks\"\n\n# Create dataset objects\ntrain_dataset = SegmentationDataset(train_image_dir, train_mask_dir, \n                                    transform=image_transform, \n                                    target_transform=mask_transform)\n\ntest_dataset = SegmentationDataset(test_image_dir, test_mask_dir, \n                                   transform=image_transform, \n                                   target_transform=mask_transform)\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)\n\n# Check a sample batch\nif __name__ == \"__main__\":\n    for images, masks in train_loader:\n        print(\"Image batch shape:\", images.shape)\n        print(\"Mask batch shape:\", masks.shape)\n        break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = UNet(input_channels=4, output_channels=1).to(device)\ndiffusion = Diffusion(T=1, device=device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loss and optimizer\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training loop\nfor epoch in range(1000):\n    model.train()\n    epoch_loss = 0\n    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        images, masks = images.to(device), masks.to(device, dtype=torch.float)  # Ensure masks are float\n\n        # Sample timestep\n        t = torch.randint(0, diffusion.T, (images.size(0),), device=device)\n        \n        # Add noise to images\n        noisy_images, noise = diffusion.q_sample(images, t)\n        \n        # Predict noise conditioned on masks\n        noise_pred = model(noisy_images, masks)\n        \n        # Compute loss\n        loss = criterion(noise_pred, masks)  # Use BCELoss with binary masks\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n\n    # Print the loss after each epoch\n    print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(train_loader):.4f}\")\n    \n    # Visualization (show images)\n    with torch.no_grad():\n        if epoch % 50 == 0:\n            # Choose a random batch for visualization\n            sample_images, sample_masks = images.cpu(), masks.cpu()\n            noisy_sample_images = noisy_images.cpu()\n            generated_images = noise_pred.cpu()\n            \n            # Plot\n            fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n            axes[0].imshow(sample_images[0, 0])\n            axes[0].set_title(\"Input Image\")\n            axes[1].imshow(noisy_sample_images[0, 0])\n            axes[1].set_title(\"Noisy Image\")\n            axes[2].imshow(generated_images[0, 0])\n            axes[2].set_title(\"Generated Image (Sigmoid)\")\n            axes[3].imshow(sample_masks[0, 0])\n            axes[3].set_title(\"Ground Truth Mask\")\n            \n            # Hide axes\n            for ax in axes:\n                ax.axis('off')\n            \n            plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to calculate IoU, Dice, Accuracy, Precision, and Recall\ndef calculate_metrics(pred, target):\n    pred = (pred > 0.5).float()  # Binarize predictions (threshold = 0.5)\n    target = target.float()\n    \n    intersection = torch.sum(pred * target)\n    union = torch.sum(pred) + torch.sum(target) - intersection\n    dice = (2.0 * intersection) / (torch.sum(pred) + torch.sum(target) + 1e-8)\n    \n    # Accuracy, Precision, Recall\n    true_positive = torch.sum(pred * target)\n    false_positive = torch.sum(pred * (1 - target))\n    false_negative = torch.sum((1 - pred) * target)\n    \n    accuracy = torch.sum(pred == target) / torch.numel(target)\n    precision = true_positive / (true_positive + false_positive + 1e-8)\n    recall = true_positive / (true_positive + false_negative + 1e-8)\n    \n    iou = intersection / (union + 1e-8)  # Avoid division by zero\n    \n    return iou.item(), dice.item(), accuracy.item(), precision.item(), recall.item()\n    \n# Evaluation function\ndef evaluate(model, dataloader, criterion, diffusion, device, visualize=False):\n    model.eval()\n    total_loss = 0\n    \n    # Initialize metrics\n    total_iou, total_dice = 0, 0\n    total_accuracy, total_precision, total_recall = 0, 0, 0\n    num_batches = 0\n    \n    with torch.no_grad():\n        for images, masks in tqdm(dataloader, desc=\"Evaluating\"):\n            images, masks = images.to(device), masks.to(device, dtype=torch.float)\n            \n            # Sample timestep\n            t = torch.randint(0, diffusion.T, (images.size(0),), device=device)\n\n            # Add noise to images\n            noisy_images, noise = diffusion.q_sample(images, t)\n\n            # Predict noise conditioned on masks\n            noise_pred = model(noisy_images, masks)\n            #noise_pred = torch.sigmoid(noise_pred)  # Apply sigmoid for probabilities\n\n            # Compute loss\n            loss = criterion(noise_pred, masks)\n            total_loss += loss.item()\n\n            # Calculate metrics\n            iou, dice, accuracy, precision, recall = calculate_metrics(noise_pred, masks)\n            total_iou += iou\n            total_dice += dice\n            total_accuracy += accuracy\n            total_precision += precision\n            total_recall += recall\n            num_batches += 1\n\n            # Visualization for the first batch\n            if visualize and num_batches == 1:\n                sample_images = images.cpu()\n                noisy_sample_images = noisy_images.cpu()\n                generated_images = noise_pred.cpu()\n                sample_masks = masks.cpu()\n\n                # Plot a few examples\n                fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n                for i in range(4):  # Show 4 samples\n                    axes[i, 0].imshow(sample_images[i, 0])\n                    axes[i, 0].set_title(\"Input Image\")\n                    axes[i, 1].imshow(noisy_sample_images[i, 0])\n                    axes[i, 1].set_title(\"Noisy Image\")\n                    axes[i, 2].imshow(generated_images[i, 0])\n                    axes[i, 2].set_title(\"Predicted Mask\")\n                    axes[i, 3].imshow(sample_masks[i, 0])\n                    axes[i, 3].set_title(\"Ground Truth Mask\")\n\n                    # Hide axes\n                    for j in range(4):\n                        axes[i, j].axis('off')\n\n                plt.tight_layout()\n                plt.show()\n\n    # Compute average metrics\n    average_loss = total_loss / num_batches\n    average_iou = total_iou / num_batches\n    average_dice = total_dice / num_batches\n    average_accuracy = total_accuracy / num_batches\n    average_precision = total_precision / num_batches\n    average_recall = total_recall / num_batches\n\n    print(f\"Average Evaluation Loss: {average_loss:.4f}\")\n    print(f\"Average IoU: {average_iou:.4f}\")\n    print(f\"Average Dice Coefficient: {average_dice:.4f}\")\n    print(f\"Average Accuracy: {average_accuracy:.4f}\")\n    print(f\"Average Precision: {average_precision:.4f}\")\n    print(f\"Average Recall: {average_recall:.4f}\")\n    \n    return {\n        \"loss\": average_loss,\n        \"iou\": average_iou,\n        \"dice\": average_dice,\n        \"accuracy\": average_accuracy,\n        \"precision\": average_precision,\n        \"recall\": average_recall\n    }\n\n\n# Perform evaluation with visualization\nmetrics = evaluate(model, test_loader, criterion, diffusion, device, visualize=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}